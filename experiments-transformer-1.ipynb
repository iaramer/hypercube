{"cells":[{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5101,"status":"ok","timestamp":1677255511772,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"lPiNHVNfJt8H","outputId":"c10b2a6d-eca6-4473-b7b9-88a33c0d7c78"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.10.0)\n","Requirement already satisfied: dobbi in /usr/local/lib/python3.8/dist-packages (0.13)\n","Requirement already satisfied: eli5 in /usr/local/lib/python3.8/dist-packages (0.13.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.8/dist-packages (from eli5) (1.0.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from eli5) (1.15.0)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.8/dist-packages (from eli5) (0.8.10)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from eli5) (1.7.3)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (from eli5) (0.10.1)\n","Requirement already satisfied: attrs>17.1.0 in /usr/local/lib/python3.8/dist-packages (from eli5) (22.2.0)\n","Requirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from eli5) (3.1.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2>=3.0.0->eli5) (2.0.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20->eli5) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=b410c6b898343ab7495455a1ed999f2d67d3a560d1f63c1b023aa1d7d089cdf1\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, sacremoses\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97\n"]}],"source":["!pip install transformers datasets sentencepiece sacremoses"]},{"cell_type":"markdown","metadata":{"id":"VknXqtEovJjd"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"eu5gFZFSvKPA"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8590,"status":"ok","timestamp":1677255846802,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"7dEeCU6_rGMm"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\yaram\\miniconda3\\envs\\pytorch_1\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","from datasets import load_dataset\n","import torch\n","import torch.nn as nn\n","from torch import nn, Tensor\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from typing import Optional\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score\n","import matplotlib.pyplot as plt\n","import time\n","from IPython.display import clear_output\n","from tqdm import tqdm\n","from transformers import BertTokenizerFast\n","import gc\n","import math"]},{"cell_type":"markdown","metadata":{"id":"KwCbTmsnvqPs"},"source":["## Constants"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":573,"status":"ok","timestamp":1677255850661,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"mM4KdBjQvN6F"},"outputs":[],"source":["RANDOM_STATE = 42\n","BATCH_SIZE = 32\n","\n","BERT_MODEL = 'bert-base-uncased'\n","HF_MODEL_HUB = 'huggingface/pytorch-transformers'"]},{"cell_type":"markdown","metadata":{"id":"3Xa94u0GJhCA"},"source":["## Hardware configuration"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1677255853147,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"SLZYnDLiJhCA","outputId":"a9935b00-9d47-472d-c224-32646f092e0a"},"outputs":[{"data":{"text/plain":["['NVIDIA GeForce GTX 1660 Ti with Max-Q Design']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()\n","torch.cuda.empty_cache()\n","\n","[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":798,"status":"ok","timestamp":1677255854756,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"mzNlMG6bJhCB","outputId":"b9ae2085-14da-471f-89a9-d159d054612b"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","device = torch.device(\"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{"id":"CMxLV3MRvU4V"},"source":["## Functions"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677255856479,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"mY1jkvyrJhCB"},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":219,"status":"ok","timestamp":1677255857447,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"7aviZZcTJhCC"},"outputs":[],"source":["def tokenize(texts, tokenizer):\n","    res = tokenizer(\n","        texts, \n","        return_tensors=\"pt\",\n","        padding='max_length',\n","        max_length=512,\n","        truncation=True\n","    )\n","    return res['input_ids'], res['attention_mask']"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677255857447,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"MfndGw4yJhCC"},"outputs":[],"source":["def train(device, model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None, n_step=100):\n","    model.train()\n","    epoch_loss = 0\n","    history = []\n","    for i, batch in enumerate(iterator):\n","        X = batch[0].to(device)\n","        y = batch[1].to(device)\n","        optimizer.zero_grad()\n","\n","        output = model(X).view(-1)\n","        loss = criterion(output, y)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","        history.append(loss.cpu().data.numpy())\n","        if (i+1)%n_step==0:\n","            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n","\n","            clear_output(True)\n","            ax[0].plot(history, label='train loss')\n","            ax[0].set_xlabel('Batch')\n","            ax[0].set_title('Train loss')\n","            if train_history is not None:\n","                ax[1].plot(train_history, label='general train history')\n","                ax[1].set_xlabel('Epoch')\n","            if valid_history is not None:\n","                ax[1].plot(valid_history, label='general valid history')\n","            plt.legend()\n","            plt.show()\n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677255859281,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"6r1usiw7JhCD"},"outputs":[],"source":["def evaluate(device, model, iterator, criterion):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            X = batch[0].to(device)\n","            y = batch[1].to(device)\n","            output = model(X).view(-1)\n","            loss = criterion(output, y)\n","            epoch_loss += loss.item()\n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":613,"status":"ok","timestamp":1677255861431,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"86hMsRFvJhCD"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677255861646,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"Zw6sa-KHJhCE"},"outputs":[],"source":["def predict(device, model, iterator):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for i, batch in tqdm(enumerate(iterator)):\n","            X = batch[0].to(device)\n","            y = batch[1].to(device)\n","            output = model(X).view(-1)\n","            y_pred += output.cpu().numpy().tolist()\n","            y_true += y.cpu().numpy().tolist()\n","    return y_true, y_pred"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677255857676,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"PkeprHL-JhCC"},"outputs":[],"source":["# def train_transformer(device, model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None, n_step=100):\n","#     model.train()\n","#     epoch_loss = 0\n","#     history = []\n","#     for i, batch in enumerate(iterator):\n","#         X = batch[0]\n","#         X_mask = batch[1]\n","#         y = batch[2]\n","#         optimizer.zero_grad()\n","\n","#         output = model(X, X_mask).view(-1)\n","#         loss = criterion(output, y)\n","#         loss.backward()\n","\n","#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","#         optimizer.step()\n","#         epoch_loss += loss.item()\n","\n","#         history.append(loss.cpu().data.numpy())\n","#         if (i+1)%n_step==0:\n","#             fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n","\n","#             clear_output(True)\n","#             ax[0].plot(history, label='train loss')\n","#             ax[0].set_xlabel('Batch')\n","#             ax[0].set_title('Train loss')\n","#             if train_history is not None:\n","#                 ax[1].plot(train_history, label='general train history')\n","#                 ax[1].set_xlabel('Epoch')\n","#             if valid_history is not None:\n","#                 ax[1].plot(valid_history, label='general valid history')\n","#             plt.legend()\n","#             plt.show()\n","#     return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677255860262,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"oXFpNkIWJhCD"},"outputs":[],"source":["# def evaluate_transformer(model, iterator, criterion):\n","#     model.eval()\n","#     epoch_loss = 0\n","#     with torch.no_grad():\n","#         for i, batch in enumerate(iterator):\n","#             X = batch[0]\n","#             X_mask = batch[1]\n","#             y = batch[2]\n","#             output = model(X, X_mask).view(-1)\n","#             loss = criterion(output, y)\n","#             epoch_loss += loss.item()\n","#     return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677255862232,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"NUE7OkvFJhCE"},"outputs":[],"source":["# def predict_transformer(model, iterator):\n","#     model.eval()\n","#     y_true = []\n","#     y_pred = []\n","#     with torch.no_grad():\n","#         for i, batch in enumerate(iterator):\n","#             X = batch[0]\n","#             X_mask = batch[1]\n","#             y = batch[2]\n","#             output = model(X, X_mask).view(-1)\n","#             y_pred += output.cpu().numpy().tolist()\n","#             y_true += y.cpu().numpy().tolist()\n","#     return y_true, y_pred"]},{"cell_type":"markdown","metadata":{"id":"zU0-OWMmwEtv"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"_q3sCHU0JhCF"},"source":["## Load"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["46436f3c3e8b44a5bfd8fd62d0eb3580","0a2e6a67cdef4501b29816b9b1d98933","9f66e76b4b2a4ce4a3301bf229f52c8f","db8f1f3a5bf44a9c9a9c0a15f628ef35","7a36956ecc374af2b9e13b51ec8aa175","ef459da4032843c1ad65113e9d6ca3f1","216a8e136a7041b0b4a22b51c4596f07","b5417078935744098280d11b22429302","a9e6e5b4a4d74f8e9d58fa07ccd6c174","961d77c64080401c8211a99147a252a7","4f29608e4d6c479c906058ec39526e8b"]},"executionInfo":{"elapsed":1041,"status":"ok","timestamp":1677255865744,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"xt_RYZ9swHuj","outputId":"70ba5e59-b4af-4509-d135-ebc74b1e15a7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset imdb (C:/Users/yaram/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n","100%|██████████| 3/3 [00:00<00:00, 18.71it/s]\n"]}],"source":["IMDB_DATASET = load_dataset('imdb')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677255866008,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"VV7jMBOzJhCF","outputId":"9f465b36-54cb-4ca6-bff2-6653b630ec0b"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["IMDB_DATASET"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677255866475,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"EXYBHf1mJhCG","outputId":"14d6ad8f-b9e3-46f7-a021-ef19163cfec2"},"outputs":[{"data":{"text/plain":["{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n"," 'label': 0}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["IMDB_DATASET['train'][0]"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1469,"status":"ok","timestamp":1677255868240,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"J8RbUcniJhCG"},"outputs":[],"source":["df_train = pd.DataFrame(IMDB_DATASET['train']).copy()\n","df_train, df_val = train_test_split(\n","    df_train, test_size=0.2, \n","    random_state=RANDOM_STATE, \n","    stratify=df_train['label']\n",")\n","\n","df_test = pd.DataFrame(IMDB_DATASET['test']).copy()"]},{"cell_type":"markdown","metadata":{"id":"70mdb8KSJhCW"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":218,"status":"ok","timestamp":1677255885184,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"bEvGEZBCJhCW"},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54072,"status":"ok","timestamp":1677255941922,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"LaK-xbrVJhCW","outputId":"caec0e70-d44d-4e1a-b6cc-e1ead745c8d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 2min 28s\n","Wall time: 43 s\n"]}],"source":["%%time\n","df_train_inputs, df_train_mask = tokenize(list(df_train['text']), tokenizer)\n","df_val_inputs, df_val_mask = tokenize(list(df_val['text']), tokenizer)\n","df_test_inputs, df_test_mask = tokenize(list(df_test['text']), tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"4UtJbhBxJhCX"},"source":["### ToTensor"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1677255944943,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"QaKTgk6ZJhCX","outputId":"2a45df0f-4d14-451f-d7a4-73cc9772c8d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 0 ns\n","Wall time: 48.2 ms\n"]}],"source":["%%time\n","\n","# convert the data to torch tensors\n","train_labels = torch.tensor(df_train['label'].to_numpy(), dtype=torch.float32)\n","valid_labels = torch.tensor(df_val['label'].to_numpy(), dtype=torch.float32)\n","test_labels = torch.tensor(df_test['label'].to_numpy(), dtype=torch.float32)\n","\n","# create TensorDataset\n","# train_dataset = TensorDataset(df_train_inputs, df_train_mask, train_labels)\n","# valid_dataset = TensorDataset(df_val_inputs, df_val_mask, valid_labels)\n","# test_dataset = TensorDataset(df_test_inputs, df_test_mask, test_labels)\n","train_dataset = TensorDataset(df_train_inputs, train_labels)\n","valid_dataset = TensorDataset(df_val_inputs, valid_labels)\n","test_dataset = TensorDataset(df_test_inputs, test_labels)\n","\n","# create dataloader\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","loaders = {\n","    \"train\": train_dataloader,\n","    \"val\": valid_dataloader,\n","}"]},{"cell_type":"markdown","metadata":{"id":"GDNYoYgoJhCW"},"source":["# Transformer implementation"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2444,"status":"ok","timestamp":1677255887852,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"rYemyCzCJhCW","outputId":"89cae924-6d51-41d5-cb0a-a9a5c938d15d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\yaram/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["bert_model = torch.hub.load(HF_MODEL_HUB, 'model', BERT_MODEL)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["bert_model"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer Modules"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["class PositionalEmbedding(nn.Module):\n","    def __init__(self, max_seq_len, embed_model_dim):\n","        super(PositionalEmbedding, self).__init__()\n","        self.embed_dim = embed_model_dim\n","\n","        pe = torch.zeros(max_seq_len, self.embed_dim)\n","        for pos in range(max_seq_len):\n","            for i in range(0, self.embed_dim, 2):\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x * math.sqrt(self.embed_dim)\n","        seq_len = x.size(1)\n","        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n","        return x"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["class ScaleDotProductAttention(nn.Module):\n","    \"\"\"\n","    Compute scale dot product attention\n","\n","    Query : given sentence that we focused on (decoder)\n","    Key : every sentence to check relationship with Qeury(encoder)\n","    Value : every sentence same with Key (encoder)\n","    \"\"\"\n","    def __init__(self):\n","        super(ScaleDotProductAttention, self).__init__()\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, q, k, v, mask=None):\n","        batch_size, head, length, d_tensor = k.size()\n","        k_t = k.transpose(2, 3)\n","        score = (q @ k_t) / torch.sqrt(torch.tensor(d_tensor))\n","        if mask is not None:\n","            score = score.masked_fill(mask == 0, -10000)\n","        score = self.softmax(score)\n","        v = score @ v\n","        return v, score"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.n_heads = n_heads\n","        self.attention = ScaleDotProductAttention()\n","        self.w_q = nn.Linear(self.input_size, self.hidden_size)\n","        self.w_k = nn.Linear(self.input_size, self.hidden_size)\n","        self.w_v = nn.Linear(self.input_size, self.hidden_size)\n","        self.w_concat = nn.Linear(self.hidden_size, self.hidden_size)\n","\n","    def forward(self, q, k, v, mask=None):\n","        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n","        q, k, v = self.split(q), self.split(k), self.split(v)\n","        out, attention = self.attention(q, k, v, mask=mask)\n","        out = self.concat(out)\n","        out = self.w_concat(out)\n","        return out\n","\n","    def split(self, tensor):\n","        \"\"\"\n","        split tensor by number of head\n","\n","        :param tensor: [batch_size, length, d_model]\n","        :return: [batch_size, head, length, d_tensor]\n","        \"\"\"\n","        batch_size, length, d_model = tensor.size()\n","\n","        d_tensor = d_model // self.n_heads\n","        tensor = tensor.view(batch_size, length, self.n_heads, d_tensor).transpose(1, 2)\n","        # it is similar with group convolution (split by number of heads)\n","        return tensor\n","\n","    def concat(self, tensor):\n","        \"\"\"\n","        inverse function of self.split(tensor : torch.Tensor)\n","\n","        :param tensor: [batch_size, head, length, d_tensor]\n","        :return: [batch_size, length, d_model]\n","        \"\"\"\n","        batch_size, head, length, d_tensor = tensor.size()\n","        d_model = head * d_tensor\n","\n","        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n","        return tensor"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_heads, drop_prob=0.1):\n","        super(EncoderLayer, self).__init__()\n","        self.attention = MultiHeadAttention(input_size, hidden_size, n_heads)\n","        self.norm1 = nn.LayerNorm(input_size)\n","        self.dropout = nn.Dropout(p=drop_prob)\n","\n","        self.linear1 = nn.Linear(input_size, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, input_size)\n","        self.relu = nn.ReLU()\n","\n","        self.norm2 = nn.LayerNorm(input_size)\n","\n","    def forward(self, x, src_mask):\n","        _x = x\n","\n","        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n","        x = self.dropout(x)\n","        \n","        x = self.norm1(x + _x)\n","        _x = x\n","\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        x = self.dropout(x)\n","\n","        x = self.norm2(x + _x)\n","        return x"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_heads, n_layers, drop_prob=0.1):\n","        super().__init__()\n","        encoder_layers = []\n","        for _ in range(n_layers):\n","            layer = EncoderLayer(\n","                input_size=input_size,\n","                hidden_size=hidden_size,\n","                n_heads=n_heads,\n","                drop_prob=drop_prob\n","            )\n","            encoder_layers.append(layer)\n","        self.layers = nn.ModuleList(encoder_layers)\n","\n","    def forward(self, x, src_mask):\n","        for layer in self.layers:\n","            x = layer(x, src_mask)\n","        return x"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["class BinaryClassificationTransformerModel(nn.Module):\n","    def __init__(self, ntoken: int, model_size: int = 128, n_heads: int = 4, \n","    nlayers: int = 1, dropout: float = 0.1, maxlen: int = 512):\n","        super().__init__()\n","        self.model_size = model_size\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEmbedding(maxlen, model_size)\n","        self.emb = nn.Embedding(ntoken, model_size)\n","        self.transformer_encoder = Encoder(\n","            input_size=self.model_size, \n","            hidden_size=self.model_size, \n","            n_heads=n_heads, \n","            n_layers=nlayers, \n","            drop_prob=dropout\n","        )\n","        self.decoder = nn.Linear(model_size, 1)  # Bin classifier\n","        self.init_weights()\n","\n","    def init_weights(self) -> None:\n","        initrange = 0.1\n","        self.emb.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n","        src = self.emb(src)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        pooled = output.mean(dim=1)\n","        output = self.decoder(pooled)\n","        return torch.sigmoid(output)"]},{"cell_type":"markdown","metadata":{"id":"1x6zR4fwJhCX"},"source":["## Base"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2837,"status":"ok","timestamp":1677255952149,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"1paxAWPRJhCY"},"outputs":[],"source":["hidden_dim = 128\n","# hidden_dim = 768\n","\n","model = BinaryClassificationTransformerModel(\n","    ntoken=30522, \n","    model_size=hidden_dim, \n","    n_heads=4, \n","    # d_hid=4*hidden_dim,\n","    nlayers=6\n",")\n","# model.emb = bert_model.embeddings.word_embeddings\n","model = model.to(device)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677255952491,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"NUg9x1jLJhCY","outputId":"5f06c3ca-5d40-4340-d875-53bd07345972"},"outputs":[{"data":{"text/plain":["4504449"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(model)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":842,"status":"ok","timestamp":1677255959236,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"2go_I3iZJhCY","outputId":"618f9358-ee6d-4b75-e999-9cd240f93438"},"outputs":[{"data":{"text/plain":["3906816"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(model.emb)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1677255961834,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"kutpT6JvJhCY"},"outputs":[],"source":["train_history = []\n","valid_history = []\n","\n","N_EPOCHS = 10\n","CLIP = 1\n","learning_rate = 0.0001\n","\n","best_valid_loss = float('inf')\n","early_stopping_counter = 0\n","early_stopping_criteria = 3\n","lr_on_plateau_update = 0.2\n","min_lr = 1e-6\n","\n","# criterion = nn.CrossEntropyLoss()\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"id":"haHfzZE-JhCY","outputId":"d6ade7c0-2f17-447e-c252-03841663110a"},"outputs":[],"source":["%%time\n","st = time.time()\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    # train_loss = train_transformer(model, train_dataloader, optimizer, criterion, CLIP, train_history, valid_history)\n","    # valid_loss = evaluate_transformer(model, valid_dataloader, criterion)\n","    train_loss = train(device, model, train_dataloader, optimizer, criterion, CLIP, train_history, valid_history)\n","    valid_loss = evaluate(device, model, valid_dataloader, criterion)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'transformer.pt')\n","        early_stopping_counter = 0\n","    else:\n","        early_stopping_counter += 1\n","        learning_rate = max(min_lr, learning_rate * lr_on_plateau_update)\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","        print(f\"LR on pleataeu update. New LR: {learning_rate}\")\n","    train_history.append(train_loss)\n","    valid_history.append(valid_loss)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    if early_stopping_counter >= early_stopping_criteria:\n","        print(f\"Early stopping, reached limit: {early_stopping_criteria}\")\n","        break\n","tt_m, tt_s = epoch_time(st, time.time())\n","\n","print(f'Total training time: {tt_m}m {tt_s}s')\n","model.load_state_dict(torch.load('transformer.pt'));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbVDQa7QJhCY","outputId":"43b93f31-8fd9-4bb3-fb57-dc249eac4b3c"},"outputs":[{"name":"stderr","output_type":"stream","text":["0it [00:00, ?it/s]c:\\Users\\yaram\\miniconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","807it [22:59,  1.71s/it]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.5\n","F1(micro): 0.5\n","F1(macro): 0.3333333333333333\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["y_true, y_pred = predict(device, model, test_dataloader)\n","y_pred = list(map(round, y_pred))\n","\n","acc = accuracy_score(y_true, y_pred)\n","f1_micro = f1_score(y_true, y_pred, average='micro')\n","f1_macro = f1_score(y_true, y_pred, average='macro')\n","\n","print(f\"Accuracy: {acc}\\nF1(micro): {f1_micro}\\nF1(macro): {f1_macro}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Reflection layer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ReflectionLayer(nn.Module):\n","    def __init__(self, inut_size, drop_prob=0.1):\n","        super(ReflectionLayer, self).__init__()\n","        self.linear = nn.Linear(input_size, input_size)\n","        self.sigm = nn.Sigmoid()\n","        self.dropout = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        gate = self.sigm(x)\n","        x = x * gate\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EncoderReflectionLayer(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_heads, drop_prob=0.1):\n","        super(EncoderReflectionLayer, self).__init__()\n","        self.attention = MultiHeadAttention(input_size, hidden_size, n_heads)\n","        self.reflection = ReflectionLayer(input_size)\n","        self.norm1 = nn.LayerNorm(input_size)\n","        self.dropout = nn.Dropout(p=drop_prob)\n","\n","        self.linear1 = nn.Linear(input_size, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, input_size)\n","        self.relu = nn.ReLU()\n","\n","        self.norm2 = nn.LayerNorm(input_size)\n","\n","    def forward(self, x, src_mask):\n","        _x = x\n","\n","        x = self.reflection(x)\n","        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n","        x = self.dropout(x)\n","        \n","        x = self.norm1(x + _x)\n","        _x = x\n","\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        x = self.dropout(x)\n","\n","        x = self.norm2(x + _x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ReflectionEncoder(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_heads, n_layers, drop_prob=0.1):\n","        super().__init__()\n","        encoder_layers = []\n","        for _ in range(n_layers):\n","            layer = EncoderReflectionLayer(\n","                input_size=input_size,\n","                hidden_size=hidden_size,\n","                n_heads=n_heads,\n","                drop_prob=drop_prob\n","            )\n","            encoder_layers.append(layer)\n","        self.layers = nn.ModuleList(encoder_layers)\n","\n","    def forward(self, x, src_mask):\n","        for layer in self.layers:\n","            x = layer(x, src_mask)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class BinaryClassificationReflectionTransformerModel(nn.Module):\n","    def __init__(self, ntoken: int, model_size: int = 128, n_heads: int = 4, \n","    nlayers: int = 1, dropout: float = 0.1, maxlen: int = 512):\n","        super().__init__()\n","        self.model_size = model_size\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEmbedding(maxlen, model_size)\n","        self.emb = nn.Embedding(ntoken, model_size)\n","        self.transformer_encoder = ReflectionEncoder(\n","            input_size=self.model_size, \n","            hidden_size=self.model_size, \n","            n_heads=n_heads, \n","            n_layers=nlayers, \n","            drop_prob=dropout\n","        )\n","        self.decoder = nn.Linear(model_size, 1)  # Bin classifier\n","        self.init_weights()\n","\n","    def init_weights(self) -> None:\n","        initrange = 0.1\n","        self.emb.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n","        src = self.emb(src)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        pooled = output.mean(dim=1)\n","        output = self.decoder(pooled)\n","        return torch.sigmoid(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2837,"status":"ok","timestamp":1677255952149,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"1paxAWPRJhCY"},"outputs":[],"source":["hidden_dim = 128\n","# hidden_dim = 768\n","\n","model = BinaryClassificationReflectionTransformerModel(\n","    ntoken=30522, \n","    model_size=hidden_dim, \n","    n_heads=4, \n","    # d_hid=4*hidden_dim,\n","    nlayers=6\n",")\n","# model.emb = bert_model.embeddings.word_embeddings\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677255952491,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"NUg9x1jLJhCY","outputId":"5f06c3ca-5d40-4340-d875-53bd07345972"},"outputs":[{"data":{"text/plain":["4504449"]},"metadata":{},"output_type":"display_data"}],"source":["count_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":842,"status":"ok","timestamp":1677255959236,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"2go_I3iZJhCY","outputId":"618f9358-ee6d-4b75-e999-9cd240f93438"},"outputs":[{"data":{"text/plain":["3906816"]},"metadata":{},"output_type":"display_data"}],"source":["count_parameters(model.emb)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1677255961834,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"kutpT6JvJhCY"},"outputs":[],"source":["train_history = []\n","valid_history = []\n","\n","N_EPOCHS = 10\n","CLIP = 1\n","learning_rate = 0.0001\n","\n","best_valid_loss = float('inf')\n","early_stopping_counter = 0\n","early_stopping_criteria = 3\n","lr_on_plateau_update = 0.2\n","min_lr = 1e-6\n","\n","# criterion = nn.CrossEntropyLoss()\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"id":"haHfzZE-JhCY","outputId":"d6ade7c0-2f17-447e-c252-03841663110a"},"outputs":[],"source":["%%time\n","st = time.time()\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    # train_loss = train_transformer(model, train_dataloader, optimizer, criterion, CLIP, train_history, valid_history)\n","    # valid_loss = evaluate_transformer(model, valid_dataloader, criterion)\n","    train_loss = train(device, model, train_dataloader, optimizer, criterion, CLIP, train_history, valid_history)\n","    valid_loss = evaluate(device, model, valid_dataloader, criterion)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'transformer.pt')\n","        early_stopping_counter = 0\n","    else:\n","        early_stopping_counter += 1\n","        learning_rate = max(min_lr, learning_rate * lr_on_plateau_update)\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","        print(f\"LR on pleataeu update. New LR: {learning_rate}\")\n","    train_history.append(train_loss)\n","    valid_history.append(valid_loss)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    if early_stopping_counter >= early_stopping_criteria:\n","        print(f\"Early stopping, reached limit: {early_stopping_criteria}\")\n","        break\n","tt_m, tt_s = epoch_time(st, time.time())\n","\n","print(f'Total training time: {tt_m}m {tt_s}s')\n","model.load_state_dict(torch.load('transformer.pt'));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbVDQa7QJhCY","outputId":"43b93f31-8fd9-4bb3-fb57-dc249eac4b3c"},"outputs":[{"name":"stderr","output_type":"stream","text":["0it [00:00, ?it/s]c:\\Users\\yaram\\miniconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","807it [22:59,  1.71s/it]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.5\n","F1(micro): 0.5\n","F1(macro): 0.3333333333333333\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["y_true, y_pred = predict(device, model, test_dataloader)\n","y_pred = list(map(round, y_pred))\n","\n","acc = accuracy_score(y_true, y_pred)\n","f1_micro = f1_score(y_true, y_pred, average='micro')\n","f1_macro = f1_score(y_true, y_pred, average='macro')\n","\n","print(f\"Accuracy: {acc}\\nF1(micro): {f1_micro}\\nF1(macro): {f1_macro}\")"]},{"cell_type":"markdown","metadata":{"id":"z7Tc3vd4JhCZ"},"source":["## HyperCube Layer"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[],"source":["class HyperCubeLayer(nn.Module):\n","    __constants__ = ['in_features', 'out_sqrt_features']\n","    in_features: int\n","    out_sqrt_features: int\n","    weight: torch.Tensor\n","\n","    def __init__(self, in_features: int, out_sqrt_features: int, bias: bool = True,\n","                 device=None, dtype=None) -> None:\n","        factory_kwargs = {'device': device, 'dtype': dtype}\n","        super().__init__()\n","        hc_input_size = np.sqrt(in_features)\n","        assert hc_input_size % 1 == 0\n","        self.hc_input_size = hc_input_size = int(hc_input_size)\n","        self.in_features = in_features\n","        self.out_sqrt_features = out_sqrt_features  # No. of output features = out_sqrt_features * sqrt(in_features)\n","        self.weight = nn.Parameter(torch.empty((out_sqrt_features*hc_input_size, hc_input_size), **factory_kwargs))\n","        if bias:\n","            self.bias = nn.Parameter(torch.empty((out_sqrt_features*hc_input_size,), **factory_kwargs))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self) -> None:\n","        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n","        if self.bias is not None:\n","            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n","            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n","            nn.init.uniform_(self.bias, -bound, bound)\n","\n","    def extra_repr(self) -> str:\n","        return 'in_features={}, hc_input_size={}, out_sqrt_features={}, bias={}'.format(\n","            self.in_features, self.hc_input_size, self.out_sqrt_features, self.bias is not None\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x.view((*x.shape[:-1], self.hc_input_size, self.hc_input_size))\n","        # For the 1st hc should probably transpose, but I'm not sure\n","        x = x.expand((self.out_sqrt_features, *x.shape))\n","        x = x.movedim(0,-3)\n","        x = x.flatten(start_dim=-3, end_dim=-2)\n","        x = x * self.weight\n","        x = torch.sum(x, axis=-1) + self.bias\n","        return x"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[],"source":["class HyperCubeBlock(nn.Module):\n","    def __init__(self, input_size, out_sqrt_features=None):\n","        if out_sqrt_features is None:\n","            out_sqrt_features = input_size\n","        super(HyperCubeBlock, self).__init__()\n","        self.hc_layers_1 = HyperCubeLayer(input_size, int(np.sqrt(input_size)))  # TODO: fix\n","        self.hc_layers_2 = HyperCubeLayer(input_size, out_sqrt_features)\n","            \n","    def forward(self, x):\n","        x = self.hc_layers_1(x)\n","        # x = x.transpose(1,2)  # !Check if needed\n","        x = self.hc_layers_2(x)\n","        return x"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["class MultiHeadAttentionHC(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_heads):\n","        super(MultiHeadAttentionHC, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.n_heads = n_heads\n","        self.attention = ScaleDotProductAttention()\n","        self.hidden_size = self.input_size  # TMP second param is also input_size instead of hidden_size\n","        # self.w_q = HyperCubeBlock(self.input_size, self.hidden_size)\n","        # self.w_k = HyperCubeBlock(self.input_size, self.hidden_size)\n","        # self.w_v = HyperCubeBlock(self.input_size, self.hidden_size)\n","        self.w_q = HyperCubeBlock(self.input_size, int(np.sqrt(self.hidden_size)))\n","        self.w_k = HyperCubeBlock(self.input_size, int(np.sqrt(self.hidden_size)))\n","        self.w_v = HyperCubeBlock(self.input_size, int(np.sqrt(self.hidden_size)))\n","        self.w_concat = HyperCubeBlock(self.hidden_size, int(np.sqrt(self.hidden_size)))\n","\n","    def forward(self, q, k, v, mask=None):\n","        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n","        q, k, v = self.split(q), self.split(k), self.split(v)\n","        out, attention = self.attention(q, k, v, mask=mask)\n","        out = self.concat(out)\n","        out = self.w_concat(out)\n","        return out\n","\n","    def split(self, tensor):\n","        \"\"\"\n","        split tensor by number of head\n","\n","        :param tensor: [batch_size, length, d_model]\n","        :return: [batch_size, head, length, d_tensor]\n","        \"\"\"\n","        batch_size, length, d_model = tensor.size()\n","\n","        d_tensor = d_model // self.n_heads\n","        tensor = tensor.view(batch_size, length, self.n_heads, d_tensor).transpose(1, 2)\n","        # it is similar with group convolution (split by number of heads)\n","        return tensor\n","\n","    def concat(self, tensor):\n","        \"\"\"\n","        inverse function of self.split(tensor : torch.Tensor)\n","\n","        :param tensor: [batch_size, head, length, d_tensor]\n","        :return: [batch_size, length, d_model]\n","        \"\"\"\n","        batch_size, head, length, d_tensor = tensor.size()\n","        d_model = head * d_tensor\n","\n","        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n","        return tensor"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[],"source":["class EncoderLayerHC(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_heads, drop_prob=0.1):\n","        super(EncoderLayerHC, self).__init__()\n","        self.attention = MultiHeadAttentionHC(input_size, hidden_size, n_heads)\n","        self.norm1 = nn.LayerNorm(input_size)\n","        self.dropout = nn.Dropout(p=drop_prob)\n","\n","        hidden_size = input_size  # TMP second param is also input_size instead of hidden_size\n","        # self.linear1 = HyperCubeBlock(input_size, hidden_size)\n","        # self.linear2 = HyperCubeBlock(hidden_size, input_size)\n","        self.linear1 = HyperCubeBlock(input_size, int(np.sqrt(hidden_size)))\n","        self.linear2 = HyperCubeBlock(hidden_size, int(np.sqrt(input_size)))\n","        self.relu = nn.ReLU()\n","\n","        self.norm2 = nn.LayerNorm(input_size)\n","\n","    def forward(self, x, src_mask):\n","        _x = x\n","\n","        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n","        x = self.dropout(x)\n","        \n","        x = self.norm1(x + _x)\n","        _x = x\n","\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        x = self.dropout(x)\n","\n","        x = self.norm2(x + _x)\n","        return x"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":["class EncoderHC(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_heads, n_layers, drop_prob=0.1):\n","        super().__init__()\n","        encoder_layers = []\n","        for _ in range(n_layers):\n","            layer = EncoderLayerHC(\n","                input_size=input_size,\n","                hidden_size=hidden_size,\n","                n_heads=n_heads,\n","                drop_prob=drop_prob\n","            )\n","            encoder_layers.append(layer)\n","        self.layers = nn.ModuleList(encoder_layers)\n","\n","    def forward(self, x, src_mask):\n","        for layer in self.layers:\n","            x = layer(x, src_mask)\n","        return x"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[],"source":["class BinaryClassificationTransformerModelHC(nn.Module):\n","    def __init__(self, ntoken: int, model_size: int = 128, n_heads: int = 4, \n","    nlayers: int = 1, dropout: float = 0.1, maxlen: int = 512):\n","        super().__init__()\n","        self.model_size = model_size\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEmbedding(maxlen, model_size)\n","        self.emb = nn.Embedding(ntoken, model_size)\n","        self.transformer_encoder = EncoderHC(\n","            input_size=self.model_size, \n","            hidden_size=self.model_size, \n","            n_heads=n_heads, \n","            n_layers=nlayers, \n","            drop_prob=dropout\n","        )\n","        self.decoder = nn.Linear(model_size, 1)  # Bin classifier\n","        self.init_weights()\n","\n","    def init_weights(self) -> None:\n","        initrange = 0.1\n","        self.emb.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n","        src = self.emb(src)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        pooled = output.mean(dim=1)\n","        output = self.decoder(pooled)\n","        return torch.sigmoid(output)"]},{"cell_type":"code","execution_count":136,"metadata":{"executionInfo":{"elapsed":2837,"status":"ok","timestamp":1677255952149,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"1paxAWPRJhCY"},"outputs":[],"source":["hidden_dim = 256\n","# hidden_dim = 768\n","\n","model = BinaryClassificationTransformerModelHC(\n","    ntoken=30522, \n","    model_size=hidden_dim, \n","    n_heads=4, \n","    # d_hid=4*hidden_dim,\n","    nlayers=6\n",")\n","# model.emb = bert_model.embeddings.word_embeddings\n","model = model.to(device)"]},{"cell_type":"code","execution_count":137,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677255952491,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"NUg9x1jLJhCY","outputId":"5f06c3ca-5d40-4340-d875-53bd07345972"},"outputs":[{"data":{"text/plain":["39998465"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(model)"]},{"cell_type":"code","execution_count":138,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":842,"status":"ok","timestamp":1677255959236,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"2go_I3iZJhCY","outputId":"618f9358-ee6d-4b75-e999-9cd240f93438"},"outputs":[{"data":{"text/plain":["31254528"]},"execution_count":138,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(model.emb)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1677255961834,"user":{"displayName":"Iaroslav Amerkhanov","userId":"14334632674182501434"},"user_tz":-60},"id":"kutpT6JvJhCY"},"outputs":[],"source":["train_history = []\n","valid_history = []\n","\n","N_EPOCHS = 10\n","CLIP = 1\n","learning_rate = 0.0001\n","\n","best_valid_loss = float('inf')\n","early_stopping_counter = 0\n","early_stopping_criteria = 3\n","lr_on_plateau_update = 0.2\n","min_lr = 1e-6\n","\n","# criterion = nn.CrossEntropyLoss()\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"id":"haHfzZE-JhCY","outputId":"d6ade7c0-2f17-447e-c252-03841663110a"},"outputs":[],"source":["%%time\n","st = time.time()\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    # train_loss = train_transformer(model, train_dataloader, optimizer, criterion, CLIP, train_history, valid_history)\n","    # valid_loss = evaluate_transformer(model, valid_dataloader, criterion)\n","    train_loss = train(device, model, train_dataloader, optimizer, criterion, CLIP, train_history, valid_history)\n","    valid_loss = evaluate(device, model, valid_dataloader, criterion)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'transformer.pt')\n","        early_stopping_counter = 0\n","    else:\n","        early_stopping_counter += 1\n","        learning_rate = max(min_lr, learning_rate * lr_on_plateau_update)\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","        print(f\"LR on pleataeu update. New LR: {learning_rate}\")\n","    train_history.append(train_loss)\n","    valid_history.append(valid_loss)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    if early_stopping_counter >= early_stopping_criteria:\n","        print(f\"Early stopping, reached limit: {early_stopping_criteria}\")\n","        break\n","tt_m, tt_s = epoch_time(st, time.time())\n","\n","print(f'Total training time: {tt_m}m {tt_s}s')\n","model.load_state_dict(torch.load('transformer.pt'));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbVDQa7QJhCY","outputId":"43b93f31-8fd9-4bb3-fb57-dc249eac4b3c"},"outputs":[{"name":"stderr","output_type":"stream","text":["0it [00:00, ?it/s]c:\\Users\\yaram\\miniconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","807it [22:59,  1.71s/it]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.5\n","F1(micro): 0.5\n","F1(macro): 0.3333333333333333\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["y_true, y_pred = predict(device, model, test_dataloader)\n","y_pred = list(map(round, y_pred))\n","\n","acc = accuracy_score(y_true, y_pred)\n","f1_micro = f1_score(y_true, y_pred, average='micro')\n","f1_macro = f1_score(y_true, y_pred, average='macro')\n","\n","print(f\"Accuracy: {acc}\\nF1(micro): {f1_micro}\\nF1(macro): {f1_macro}\")"]},{"cell_type":"markdown","metadata":{"id":"4Nfg0XAkJhCZ"},"source":["## HyperCube Layer V2\n","\n","Optimised computation."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class HyperCubeLayer(nn.Module):\n","    __constants__ = ['in_features', 'out_sqrt_features']\n","    in_features: int\n","    out_sqrt_features: int\n","    weight: torch.Tensor\n","\n","    def __init__(self, in_features: int, out_sqrt_features: int, bias: bool = True,\n","                 device=None, dtype=None) -> None:\n","        factory_kwargs = {'device': device, 'dtype': dtype}\n","        super().__init__()\n","        hc_input_size = np.sqrt(in_features)\n","        assert hc_input_size % 1 == 0\n","        self.hc_input_size = hc_input_size = int(hc_input_size)\n","        self.in_features = in_features\n","        self.out_sqrt_features = out_sqrt_features  # No. of output features = out_sqrt_features * sqrt(in_features)\n","        self.weight = nn.Parameter(torch.empty((out_sqrt_features, hc_input_size, hc_input_size), **factory_kwargs))\n","        if bias:\n","            self.bias = nn.Parameter(torch.empty((out_sqrt_features,), **factory_kwargs))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self) -> None:\n","        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n","        if self.bias is not None:\n","            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n","            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n","            nn.init.uniform_(self.bias, -bound, bound)\n","\n","    def extra_repr(self) -> str:\n","        return 'in_features={}, hc_input_size={}, out_sqrt_features={}, bias={}'.format(\n","            self.in_features, self.hc_input_size, self.out_sqrt_features, self.bias is not None\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x.view((*x.shape[:-1], self.hc_input_size, self.hc_input_size))\n","        x = (x.movedim(1,2) @ self.weight).movedim(2,1) + self.bias\n","        x = x.flatten(start_dim=-2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"G7qkTtS-JhCa"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\yaram\\miniconda3\\envs\\pytorch_1\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Downloading: 100%|██████████| 502/502 [00:00<00:00, 167kB/s]\n","c:\\Users\\yaram\\miniconda3\\envs\\pytorch_1\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yaram\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","Downloading: 100%|██████████| 346M/346M [00:56<00:00, 6.17MB/s]   \n"]}],"source":["from transformers import AutoImageProcessor, ViTModel\n","\n","model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["ViTModel(\n","  (embeddings): ViTEmbeddings(\n","    (patch_embeddings): ViTPatchEmbeddings(\n","      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    )\n","    (dropout): Dropout(p=0.0, inplace=False)\n","  )\n","  (encoder): ViTEncoder(\n","    (layer): ModuleList(\n","      (0): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (2): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (3): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (4): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (5): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (6): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (7): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (8): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (9): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (10): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (11): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","  (pooler): ViTPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["742656"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(model.embeddings)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from transformers import MobileViTForImageClassification"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|██████████| 70.0k/70.0k [00:00<00:00, 121kB/s] \n","Downloading: 100%|██████████| 22.5M/22.5M [00:06<00:00, 3.38MB/s]\n"]}],"source":["model_ckpt = \"apple/mobilevit-small\"\n","mmodel = MobileViTForImageClassification.from_pretrained(model_ckpt)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["MobileViTForImageClassification(\n","  (mobilevit): MobileViTModel(\n","    (conv_stem): MobileViTConvLayer(\n","      (convolution): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (normalization): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): SiLUActivation()\n","    )\n","    (encoder): MobileViTEncoder(\n","      (layer): ModuleList(\n","        (0): MobileViTMobileNetLayer(\n","          (layer): ModuleList(\n","            (0): MobileViTInvertedResidual(\n","              (expand_1x1): MobileViTConvLayer(\n","                (convolution): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                (activation): SiLUActivation()\n","              )\n","              (conv_3x3): MobileViTConvLayer(\n","                (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                (activation): SiLUActivation()\n","              )\n","              (reduce_1x1): MobileViTConvLayer(\n","                (convolution): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              )\n","            )\n","          )\n","        )\n","        (1): MobileViTMobileNetLayer(\n","          (layer): ModuleList(\n","            (0): MobileViTInvertedResidual(\n","              (expand_1x1): MobileViTConvLayer(\n","                (convolution): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                (activation): SiLUActivation()\n","              )\n","              (conv_3x3): MobileViTConvLayer(\n","                (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n","                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                (activation): SiLUActivation()\n","              )\n","              (reduce_1x1): MobileViTConvLayer(\n","                (convolution): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              )\n","            )\n","            (1): MobileViTInvertedResidual(\n","              (expand_1x1): MobileViTConvLayer(\n","                (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                (activation): SiLUActivation()\n","              )\n","              (conv_3x3): MobileViTConvLayer(\n","                (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                (activation): SiLUActivation()\n","              )\n","              (reduce_1x1): MobileViTConvLayer(\n","                (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              )\n","            )\n","            (2): MobileViTInvertedResidual(\n","              (expand_1x1): MobileViTConvLayer(\n","                (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                (activation): SiLUActivation()\n","              )\n","              (conv_3x3): MobileViTConvLayer(\n","                (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                (activation): SiLUActivation()\n","              )\n","              (reduce_1x1): MobileViTConvLayer(\n","                (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              )\n","            )\n","          )\n","        )\n","        (2): MobileViTLayer(\n","          (downsampling_layer): MobileViTInvertedResidual(\n","            (expand_1x1): MobileViTConvLayer(\n","              (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (activation): SiLUActivation()\n","            )\n","            (conv_3x3): MobileViTConvLayer(\n","              (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n","              (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (activation): SiLUActivation()\n","            )\n","            (reduce_1x1): MobileViTConvLayer(\n","              (convolution): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (normalization): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (conv_kxk): MobileViTConvLayer(\n","            (convolution): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","          (conv_1x1): MobileViTConvLayer(\n","            (convolution): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          )\n","          (transformer): MobileViTTransformer(\n","            (layer): ModuleList(\n","              (0): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=144, out_features=144, bias=True)\n","                    (key): Linear(in_features=144, out_features=144, bias=True)\n","                    (value): Linear(in_features=144, out_features=144, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=144, out_features=144, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=144, out_features=288, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=288, out_features=144, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (1): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=144, out_features=144, bias=True)\n","                    (key): Linear(in_features=144, out_features=144, bias=True)\n","                    (value): Linear(in_features=144, out_features=144, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=144, out_features=144, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=144, out_features=288, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=288, out_features=144, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","          )\n","          (layernorm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n","          (conv_projection): MobileViTConvLayer(\n","            (convolution): Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","          (fusion): MobileViTConvLayer(\n","            (convolution): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","        )\n","        (3): MobileViTLayer(\n","          (downsampling_layer): MobileViTInvertedResidual(\n","            (expand_1x1): MobileViTConvLayer(\n","              (convolution): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (activation): SiLUActivation()\n","            )\n","            (conv_3x3): MobileViTConvLayer(\n","              (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n","              (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (activation): SiLUActivation()\n","            )\n","            (reduce_1x1): MobileViTConvLayer(\n","              (convolution): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (conv_kxk): MobileViTConvLayer(\n","            (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","          (conv_1x1): MobileViTConvLayer(\n","            (convolution): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          )\n","          (transformer): MobileViTTransformer(\n","            (layer): ModuleList(\n","              (0): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=192, out_features=192, bias=True)\n","                    (key): Linear(in_features=192, out_features=192, bias=True)\n","                    (value): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=192, out_features=384, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=384, out_features=192, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (1): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=192, out_features=192, bias=True)\n","                    (key): Linear(in_features=192, out_features=192, bias=True)\n","                    (value): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=192, out_features=384, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=384, out_features=192, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (2): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=192, out_features=192, bias=True)\n","                    (key): Linear(in_features=192, out_features=192, bias=True)\n","                    (value): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=192, out_features=384, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=384, out_features=192, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (3): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=192, out_features=192, bias=True)\n","                    (key): Linear(in_features=192, out_features=192, bias=True)\n","                    (value): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=192, out_features=384, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=384, out_features=192, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","          )\n","          (layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (conv_projection): MobileViTConvLayer(\n","            (convolution): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","          (fusion): MobileViTConvLayer(\n","            (convolution): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","        )\n","        (4): MobileViTLayer(\n","          (downsampling_layer): MobileViTInvertedResidual(\n","            (expand_1x1): MobileViTConvLayer(\n","              (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (activation): SiLUActivation()\n","            )\n","            (conv_3x3): MobileViTConvLayer(\n","              (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n","              (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (activation): SiLUActivation()\n","            )\n","            (reduce_1x1): MobileViTConvLayer(\n","              (convolution): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (normalization): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (conv_kxk): MobileViTConvLayer(\n","            (convolution): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","          (conv_1x1): MobileViTConvLayer(\n","            (convolution): Conv2d(160, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          )\n","          (transformer): MobileViTTransformer(\n","            (layer): ModuleList(\n","              (0): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=240, out_features=240, bias=True)\n","                    (key): Linear(in_features=240, out_features=240, bias=True)\n","                    (value): Linear(in_features=240, out_features=240, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=240, out_features=240, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=240, out_features=480, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=480, out_features=240, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (1): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=240, out_features=240, bias=True)\n","                    (key): Linear(in_features=240, out_features=240, bias=True)\n","                    (value): Linear(in_features=240, out_features=240, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=240, out_features=240, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=240, out_features=480, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=480, out_features=240, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (2): MobileViTTransformerLayer(\n","                (attention): MobileViTAttention(\n","                  (attention): MobileViTSelfAttention(\n","                    (query): Linear(in_features=240, out_features=240, bias=True)\n","                    (key): Linear(in_features=240, out_features=240, bias=True)\n","                    (value): Linear(in_features=240, out_features=240, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): MobileViTSelfOutput(\n","                    (dense): Linear(in_features=240, out_features=240, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): MobileViTIntermediate(\n","                  (dense): Linear(in_features=240, out_features=480, bias=True)\n","                  (intermediate_act_fn): SiLUActivation()\n","                )\n","                (output): MobileViTOutput(\n","                  (dense): Linear(in_features=480, out_features=240, bias=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","                (layernorm_before): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n","                (layernorm_after): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","          )\n","          (layernorm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n","          (conv_projection): MobileViTConvLayer(\n","            (convolution): Conv2d(240, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","          (fusion): MobileViTConvLayer(\n","            (convolution): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (normalization): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (activation): SiLUActivation()\n","          )\n","        )\n","      )\n","    )\n","    (conv_1x1_exp): MobileViTConvLayer(\n","      (convolution): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (normalization): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): SiLUActivation()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=True)\n","  (classifier): Linear(in_features=640, out_features=1000, bias=True)\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["mmodel"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["5578632"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(mmodel)"]},{"cell_type":"markdown","metadata":{"id":"evy6SHsrJhCa"},"source":["# "]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"nlp_lab_1k","language":"python","name":"nlp_lab_1k"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"d8d360e7c7d2a57b96b1fd1f3db2a7e7cffffe58fb2fb3aa8cedb08129f5b237"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0a2e6a67cdef4501b29816b9b1d98933":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef459da4032843c1ad65113e9d6ca3f1","placeholder":"​","style":"IPY_MODEL_216a8e136a7041b0b4a22b51c4596f07","value":"100%"}},"216a8e136a7041b0b4a22b51c4596f07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46436f3c3e8b44a5bfd8fd62d0eb3580":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a2e6a67cdef4501b29816b9b1d98933","IPY_MODEL_9f66e76b4b2a4ce4a3301bf229f52c8f","IPY_MODEL_db8f1f3a5bf44a9c9a9c0a15f628ef35"],"layout":"IPY_MODEL_7a36956ecc374af2b9e13b51ec8aa175"}},"4f29608e4d6c479c906058ec39526e8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a36956ecc374af2b9e13b51ec8aa175":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"961d77c64080401c8211a99147a252a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f66e76b4b2a4ce4a3301bf229f52c8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5417078935744098280d11b22429302","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9e6e5b4a4d74f8e9d58fa07ccd6c174","value":3}},"a9e6e5b4a4d74f8e9d58fa07ccd6c174":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5417078935744098280d11b22429302":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db8f1f3a5bf44a9c9a9c0a15f628ef35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_961d77c64080401c8211a99147a252a7","placeholder":"​","style":"IPY_MODEL_4f29608e4d6c479c906058ec39526e8b","value":" 3/3 [00:00&lt;00:00, 56.74it/s]"}},"ef459da4032843c1ad65113e9d6ca3f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
